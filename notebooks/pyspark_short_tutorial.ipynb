{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c689a0",
   "metadata": {},
   "source": [
    "# Apache Spark with Python (PySpark)\n",
    "## Short Tutorial\n",
    "\n",
    "This notebook is designed for a **short session** in the *Big Data & Business Intelligence* course.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "1. Explain what Apache Spark is and when to use it.\n",
    "2. Start a local PySpark session (`SparkSession`).\n",
    "3. Load data into Spark DataFrames.\n",
    "4. Perform basic transformations and aggregations.\n",
    "5. Use Spark SQL to query data.\n",
    "6. Build a simple ETL pipeline that reads CSV, transforms, joins, and writes Parquet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7aee3",
   "metadata": {},
   "source": [
    "## 1. What is Apache Spark?\n",
    "\n",
    "Apache Spark is a **distributed data processing framework**. It is designed for:\n",
    "\n",
    "- Handling **large datasets** that do not fit into memory on one machine.\n",
    "- Running computations **in parallel** across a cluster of machines.\n",
    "- Providing a **high-level API** (DataFrames, SQL) that feels similar to Pandas or SQL.\n",
    "\n",
    "### Understanding the Problem: Why Do We Need Spark?\n",
    "\n",
    "Imagine you work for a company that has **terabytes** of customer data like transaction logs, clickstream data, sensor readings, etc. You need to analyze this data to generate reports or train machine learning models.\n",
    "\n",
    "**The Challenge:**\n",
    "- Your laptop has maybe 8-16 GB of RAM.\n",
    "- Traditional tools like **Pandas** load all data into memory on a single machine.\n",
    "- If your dataset is 500 GB, Pandas simply **cannot handle it**—your computer will crash or freeze.\n",
    "\n",
    "**The Solution: Distributed Computing**\n",
    "\n",
    "This is where **Apache Spark** comes in. Instead of trying to process all the data on one machine, Spark:\n",
    "\n",
    "1. **Splits** the data into smaller chunks.\n",
    "2. **Distributes** these chunks across multiple machines (called a \"cluster\").\n",
    "3. **Processes** each chunk in parallel on different machines.\n",
    "4. **Combines** the results back together.\n",
    "\n",
    "This approach allows you to process datasets that are **much larger than the memory of any single machine**.\n",
    "\n",
    "### Why not just use Pandas?\n",
    "\n",
    "| **Pandas** | **Spark** |\n",
    "|------------|-----------|\n",
    "| Keeps all data in **memory on a single machine** | **Distributes** data across many machines |\n",
    "| Fast for small-to-medium datasets (< 10 GB) | Designed for **large datasets** (GBs to PBs) |\n",
    "| Single-threaded or limited parallelism | Runs computations **in parallel** across all cores/machines |\n",
    "| Runs only on your laptop | Can run locally **or** on a cloud cluster with hundreds of machines |\n",
    "\n",
    "### When Should You Use Spark?\n",
    "\n",
    "- **Large datasets** that don't fit in memory (> 10-100 GB)\n",
    "- **Complex data pipelines** (ETL: Extract, Transform, Load)\n",
    "- **Big data analytics** and reporting\n",
    "- **Machine learning** on large datasets\n",
    "- When you need to **scale** from your laptop to a cloud cluster\n",
    "\n",
    "### When Should You Use Pandas Instead?\n",
    "\n",
    "- Dataset fits comfortably in memory (< 5 GB)\n",
    "- Quick exploratory analysis\n",
    "- Simple data manipulations\n",
    "\n",
    "### Key Concepts in Spark\n",
    "\n",
    "**1. Cluster Computing**\n",
    "- A **cluster** is a group of computers (called \"nodes\") working together.\n",
    "- Spark can run on a single machine (local mode) or on a cluster (distributed mode).\n",
    "- The same Spark code runs in **both modes**—no changes needed!\n",
    "\n",
    "**2. Lazy Evaluation**\n",
    "- Spark doesn't execute operations immediately.\n",
    "- It builds a **plan** of what to do, then executes everything when you ask for results.\n",
    "- This allows Spark to **optimize** the entire workflow.\n",
    "\n",
    "**3. DataFrames and SQL**\n",
    "- In modern Spark, we work with **DataFrames** (similar to Pandas DataFrames or SQL tables).\n",
    "- You can use **SQL queries** or **DataFrame API** methods—whichever you prefer.\n",
    "- Under the hood, Spark converts everything to optimized execution plans.\n",
    "\n",
    "### Real-World Example\n",
    "\n",
    "**Scenario:** An e-commerce company analyzes 2 TB of daily clickstream data.\n",
    "\n",
    "- **With Pandas:** Impossible—data doesn't fit in memory.\n",
    "- **With Spark:** \n",
    "  - Data is split across 100 machines.\n",
    "  - Each machine processes 20 GB in parallel.\n",
    "  - Results are combined to generate insights in minutes.\n",
    "  - The same code can run on your laptop with a sample dataset for testing!\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, we will run Spark in **local mode** on your laptop, but remember: the same code can scale to process massive datasets in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fa5e0",
   "metadata": {},
   "source": [
    "## 2. Environment Setup — Starting a SparkSession\n",
    "\n",
    "> If you are running this locally and do not have PySpark installed yet, either run:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "or follow the environment setup that uses the requirements.txt file to install pyspark.\n",
    "In this notebook, we create a **SparkSession**, which is the main entry point for using Spark with DataFrames and SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2fa18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/24 23:53:59 WARN Utils: Your hostname, Aakashs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.109 instead (on interface en0)\n",
      "25/11/24 23:53:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/24 23:54:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<pyspark.sql.session.SparkSession at 0x11492a270>, '4.0.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BD_BI_Spark_Tutorial\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Suppress verbose logging output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark, spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb2096",
   "metadata": {},
   "source": [
    "### Understanding the SparkSession Configuration\n",
    "\n",
    "Let's break down what each part of the SparkSession setup means:\n",
    "\n",
    "- **`SparkSession.builder`**: Starts the configuration process for creating a Spark session.\n",
    "- **`.appName(\"BD_BI_Spark_Tutorial\")`**: Sets a name for your application. This name appears in Spark's monitoring UI and logs, making it easy to identify your job.\n",
    "- **`.master(\"local[*]\")`**: Tells Spark where to run:\n",
    "  - `local` means run on your laptop (not a cluster)\n",
    "  - `[*]` means use all available CPU cores for parallel processing\n",
    "  - You could also use `local[2]` to use only 2 cores, or `local[4]` for 4 cores\n",
    "- **`.getOrCreate()`**: Creates a new SparkSession, or reuses an existing one if it's already running (useful when re-running cells).\n",
    "- **`.sparkContext.setLogLevel(\"ERROR\")`**: Reduces verbose output by only showing error messages.\n",
    "\n",
    "**Important:** Once created, you use the `spark` object to read data, create DataFrames, and run SQL queries throughout your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db213e",
   "metadata": {},
   "source": [
    "## 3. Creating a Sample Dataset\n",
    "\n",
    "For this tutorial, we will create a small **housing dataset** directly in the notebook, instead of reading from disk.\n",
    "In a real setting, you would typically read from CSV, Parquet, or a database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcde20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(house_id=1, neighborhood_id=10, price=300000, sqft=80,  bedrooms=2, year_built=1990, price_per_bedroom=100000),\n",
    "    Row(house_id=2, neighborhood_id=10, price=450000, sqft=100, bedrooms=3, year_built=2005, price_per_bedroom=200000),\n",
    "    Row(house_id=3, neighborhood_id=11, price=500000, sqft=120, bedrooms=4, year_built=2010, price_per_bedroom=190000),\n",
    "    Row(house_id=4, neighborhood_id=11, price=200000, sqft=60,  bedrooms=2, year_built=1980, price_per_bedroom=180000),\n",
    "    Row(house_id=5, neighborhood_id=12, price=800000, sqft=150, bedrooms=5, year_built=2018 , price_per_bedroom=300000),\n",
    "]\n",
    "\n",
    "houses_df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd27e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- house_id: long (nullable = true)\n",
      " |-- neighborhood_id: long (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- sqft: long (nullable = true)\n",
      " |-- bedrooms: long (nullable = true)\n",
      " |-- year_built: long (nullable = true)\n",
      " |-- price_per_bedroom: long (nullable = true)\n",
      "\n",
      "+--------+---------------+------+----+--------+----------+-----------------+\n",
      "|house_id|neighborhood_id| price|sqft|bedrooms|year_built|price_per_bedroom|\n",
      "+--------+---------------+------+----+--------+----------+-----------------+\n",
      "|       1|             10|300000|  80|       2|      1990|           100000|\n",
      "|       2|             10|450000| 100|       3|      2005|           200000|\n",
      "|       3|             11|500000| 120|       4|      2010|           190000|\n",
      "|       4|             11|200000|  60|       2|      1980|           180000|\n",
      "|       5|             12|800000| 150|       5|      2018|           300000|\n",
      "+--------+---------------+------+----+--------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines to see the schema and data\n",
    "houses_df.printSchema()\n",
    "houses_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a89e7f",
   "metadata": {},
   "source": [
    "### DataFrame vs Pandas\n",
    "\n",
    "- A **Spark DataFrame** looks similar to a Pandas DataFrame when you `.show()` it.\n",
    "- But it is **not** actually held entirely in memory like in Pandas.\n",
    "- Spark only runs computations **lazily**: it plans the query first and only executes when needed (e.g., on `.show()`, `.count()`, `.collect()`).\n",
    "\n",
    "### Working with Large Datasets\n",
    "\n",
    "When working with very large datasets (millions or billions of rows), you should **never** try to see all the data at once. Here are the safe ways to inspect your data:\n",
    "\n",
    "**Preview the first few rows:**\n",
    "```python\n",
    "houses_df.show(5)          # Show first 5 rows (default is 20)\n",
    "houses_df.show(10, False)  # Show 10 rows without truncating wide columns\n",
    "```\n",
    "\n",
    "**Similar to Pandas `.head()`:**\n",
    "```python\n",
    "houses_df.limit(5).show()  # Equivalent to df.head(5) in Pandas\n",
    "```\n",
    "\n",
    "**What's the difference between `.show(5)` and `.limit(5).show()`?**\n",
    "\n",
    "- **`.show(5)`**: Only displays 5 rows but the DataFrame itself (`houses_df`) remains unchanged with all rows.\n",
    "- **`.limit(5).show()`**: Creates a **new DataFrame** with only 5 rows, then displays it. This new DataFrame can be saved and reused.\n",
    "\n",
    "```python\n",
    "# Example:\n",
    "top5 = houses_df.limit(5)  # Creates a new DataFrame with 5 rows\n",
    "top5.count()               # Returns 5\n",
    "houses_df.count()          # Still returns the original row count\n",
    "\n",
    "# .show(5) is just for display - it doesn't create a new DataFrame\n",
    "```\n",
    "\n",
    "Use `.show(n)` when you just want to peek at the data. Use `.limit(n)` when you need a smaller DataFrame for testing or further processing.\n",
    "\n",
    "**WARNING:** Never use `.collect()` on large datasets! This pulls ALL data into memory on your laptop and will crash it.\n",
    "```python\n",
    "# DANGEROUS with big data:\n",
    "# all_data = houses_df.collect()  # DON'T DO THIS with TB of data!\n",
    "```\n",
    "\n",
    "**Safe alternatives:**\n",
    "- Use `.show(n)` to preview a few rows\n",
    "- Use `.count()` to get the total number of rows\n",
    "- Use `.describe().show()` to get summary statistics\n",
    "- Use aggregations (`.groupBy()`, `.agg()`) to summarize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca68af2d",
   "metadata": {},
   "source": [
    "### Exercise: Inspect the data\n",
    "\n",
    "1. Use `houses_df.count()` to count the rows.\n",
    "2. Use `houses_df.describe().show()` to see basic statistics.\n",
    "3. Try `houses_df.select(\"price\", \"sqft\").show()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f0a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+----------------+-----------------+------------------+------------------+-----------------+\n",
      "|summary|          house_id|   neighborhood_id|           price|             sqft|          bedrooms|        year_built|price_per_bedroom|\n",
      "+-------+------------------+------------------+----------------+-----------------+------------------+------------------+-----------------+\n",
      "|  count|                 5|                 5|               5|                5|                 5|                 5|                5|\n",
      "|   mean|               3.0|              10.8|        450000.0|            102.0|               3.2|            2000.6|         194000.0|\n",
      "| stddev|1.5811388300841898|0.8366600265340753|229128.784747792|34.92849839314596|1.3038404810405297|15.388307249337096|71274.11872482185|\n",
      "|    min|                 1|                10|          200000|               60|                 2|              1980|           100000|\n",
      "|    max|                 5|                12|          800000|              150|                 5|              2018|           300000|\n",
      "+-------+------------------+------------------+----------------+-----------------+------------------+------------------+-----------------+\n",
      "\n",
      "+------+----+--------+\n",
      "| price|sqft|bedrooms|\n",
      "+------+----+--------+\n",
      "|300000|  80|       2|\n",
      "|450000| 100|       3|\n",
      "|500000| 120|       4|\n",
      "|200000|  60|       2|\n",
      "|800000| 150|       5|\n",
      "+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "houses_df.count()\n",
    "houses_df.describe().show()\n",
    "houses_df.select(\"price\", \"sqft\", \"bedrooms\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2c6bd",
   "metadata": {},
   "source": [
    "## 4. Basic DataFrame Operations\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- Select and filter columns.\n",
    "- Create new columns with `withColumn`.\n",
    "- Understand lazy evaluation briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4547a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+\n",
      "|house_id| price|sqft|    price_per_sqft|\n",
      "+--------+------+----+------------------+\n",
      "|       1|300000|  80|            3750.0|\n",
      "|       2|450000| 100|            4500.0|\n",
      "|       3|500000| 120| 4166.666666666667|\n",
      "|       4|200000|  60|3333.3333333333335|\n",
      "|       5|800000| 150| 5333.333333333333|\n",
      "+--------+------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add a new column: price per square meter\n",
    "houses_df = houses_df.withColumn(\"price_per_sqft\", col(\"price\") / col(\"sqft\"))\n",
    "houses_df.select(\"house_id\", \"price\", \"sqft\", \"price_per_sqft\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf33f0",
   "metadata": {},
   "source": [
    "We can also filter rows using `.filter()` or `.where()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13425f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------+----+--------+----------+-----------------+-----------------+\n",
      "|house_id|neighborhood_id| price|sqft|bedrooms|year_built|price_per_bedroom|   price_per_sqft|\n",
      "+--------+---------------+------+----+--------+----------+-----------------+-----------------+\n",
      "|       1|             10|300000|  80|       2|      1990|           100000|           3750.0|\n",
      "|       2|             10|450000| 100|       3|      2005|           200000|           4500.0|\n",
      "|       3|             11|500000| 120|       4|      2010|           190000|4166.666666666667|\n",
      "|       5|             12|800000| 150|       5|      2018|           300000|5333.333333333333|\n",
      "+--------+---------------+------+----+--------+----------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "houses_df.filter(\"sqft >69\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd9717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------+----+--------+----------+-----------------+-----------------+\n",
      "|house_id|neighborhood_id| price|sqft|bedrooms|year_built|price_per_bedroom|   price_per_sqft|\n",
      "+--------+---------------+------+----+--------+----------+-----------------+-----------------+\n",
      "|       2|             10|450000| 100|       3|      2005|           200000|           4500.0|\n",
      "|       3|             11|500000| 120|       4|      2010|           190000|4166.666666666667|\n",
      "|       5|             12|800000| 150|       5|      2018|           300000|5333.333333333333|\n",
      "+--------+---------------+------+----+--------+----------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "houses_df.where(\"price > 400000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c7e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+----+-----------------+\n",
      "|house_id|bedrooms| price|sqft|   price_per_sqft|\n",
      "+--------+--------+------+----+-----------------+\n",
      "|       3|       4|500000| 120|4166.666666666667|\n",
      "|       5|       5|800000| 150|5333.333333333333|\n",
      "+--------+--------+------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for houses with more than 3 bedrooms\n",
    "large_houses = houses_df.filter(col(\"bedrooms\") > 3)\n",
    "\n",
    "large_houses.select(\"house_id\", \"bedrooms\", \"price\", \"sqft\", \"price_per_sqft\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f4378",
   "metadata": {},
   "source": [
    "### Exercise: Create a custom metric\n",
    "\n",
    "1. Create a new column `price_per_bedroom` = `price / bedrooms`.\n",
    "2. Filter for houses where `price_per_bedroom` is less than **200,000**.\n",
    "3. Show the `house_id`, `bedrooms`, and `price_per_bedroom` columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dbd7ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------+----+--------+----------+-----------------+------------------+\n",
      "|house_id|neighborhood_id| price|sqft|bedrooms|year_built|price_per_bedroom|    price_per_sqft|\n",
      "+--------+---------------+------+----+--------+----------+-----------------+------------------+\n",
      "|       1|             10|300000|  80|       2|      1990|           100000|            3750.0|\n",
      "|       3|             11|500000| 120|       4|      2010|           190000| 4166.666666666667|\n",
      "|       4|             11|200000|  60|       2|      1980|           180000|3333.3333333333335|\n",
      "+--------+---------------+------+----+--------+----------+-----------------+------------------+\n",
      "\n",
      "+--------+--------+------------------+\n",
      "|house_id|bedrooms|    price_per_sqft|\n",
      "+--------+--------+------------------+\n",
      "|       1|       2|            3750.0|\n",
      "|       2|       3|            4500.0|\n",
      "|       3|       4| 4166.666666666667|\n",
      "|       4|       2|3333.3333333333335|\n",
      "|       5|       5| 5333.333333333333|\n",
      "+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "houses_df.filter(\"price_per_bedroom < 200000\").show()\n",
    "houses_df.select(\"house_id\", \"bedrooms\", \"price_per_sqft\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3aa08",
   "metadata": {},
   "source": [
    "## 5. Aggregations with `groupBy`\n",
    "\n",
    "Spark makes it easy to compute statistics over groups, similar to `GROUP BY` in SQL or `.groupby()` in Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a32464d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+---------+\n",
      "|bedrooms|avg_price|min_price|max_price|\n",
      "+--------+---------+---------+---------+\n",
      "|       2| 250000.0|   200000|   300000|\n",
      "|       3| 450000.0|   450000|   450000|\n",
      "|       4| 500000.0|   500000|   500000|\n",
      "|       5| 800000.0|   800000|   800000|\n",
      "+--------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, min, max\n",
    "\n",
    "# Average price per number of bedrooms\n",
    "agg_df = houses_df.groupBy(\"bedrooms\").agg(\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "    min(\"price\").alias(\"min_price\"),\n",
    "    max(\"price\").alias(\"max_price\")\n",
    ").orderBy(\"bedrooms\")\n",
    "\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79d57b",
   "metadata": {},
   "source": [
    "### Exercise: Aggregation practice\n",
    "\n",
    "1. Compute the **average** `price_per_sqft` per number of bedrooms.\n",
    "2. Sort the result by `price_per_sqft` in **descending** order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c8b8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------------+-----------------+\n",
      "|bedrooms|        avg_price|         min_price|        max_price|\n",
      "+--------+-----------------+------------------+-----------------+\n",
      "|       5|5333.333333333333| 5333.333333333333|5333.333333333333|\n",
      "|       4|4166.666666666667| 4166.666666666667|4166.666666666667|\n",
      "|       3|           4500.0|            4500.0|           4500.0|\n",
      "|       2|3541.666666666667|3333.3333333333335|           3750.0|\n",
      "+--------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, min, max, desc\n",
    "\n",
    "# Average price per number of bedrooms, sorted in descending order\n",
    "agg_df = houses_df.groupBy(\"bedrooms\").agg(\n",
    "    avg(\"price_per_sqft\").alias(\"avg_price\"),\n",
    "    min(\"price_per_sqft\").alias(\"min_price\"),\n",
    "    max(\"price_per_sqft\").alias(\"max_price\")\n",
    ").orderBy(desc(\"bedrooms\"))\n",
    "\n",
    "agg_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5644d6",
   "metadata": {},
   "source": [
    "## 6. Spark SQL\n",
    "\n",
    "You can use **SQL queries** on Spark DataFrames. First, you need to register a DataFrame as a **temporary view**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d558ba5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------------+\n",
      "|bedrooms|avg_price|avg_price_per_sqft|\n",
      "+--------+---------+------------------+\n",
      "|       5| 800000.0| 5333.333333333333|\n",
      "|       4| 500000.0| 4166.666666666667|\n",
      "|       3| 450000.0|            4500.0|\n",
      "|       2| 250000.0| 3541.666666666667|\n",
      "+--------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL view\n",
    "houses_df.createOrReplaceTempView(\"houses\")\n",
    "\n",
    "# Use SQL to compute average price per bedroom\n",
    "sql_result = spark.sql(\"\"\"\n",
    "SELECT bedrooms,\n",
    "       AVG(price) AS avg_price,\n",
    "       AVG(price_per_sqft) AS avg_price_per_sqft\n",
    "FROM houses\n",
    "GROUP BY bedrooms\n",
    "ORDER BY avg_price DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af2686",
   "metadata": {},
   "source": [
    "Spark SQL is useful because:\n",
    "\n",
    "- Many people already know **SQL**, so they can work with data quickly.\n",
    "- The same query can run on **very large** datasets in a cluster.\n",
    "- Spark can automatically optimize SQL queries internally.\n",
    "\n",
    "### Exercise: Your own SQL query\n",
    "\n",
    "Write a SQL query that:\n",
    "\n",
    "1. Selects `house_id`, `price`, `bedrooms`, `price_per_sqft`.\n",
    "2. Only keeps rows where `bedrooms >= 3`.\n",
    "3. Orders the results by `price_per_sqft` **ascending**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a83ca1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------------+\n",
      "|bedrooms|avg_price|avg_price_per_sqft|\n",
      "+--------+---------+------------------+\n",
      "|       5| 800000.0| 5333.333333333333|\n",
      "|       4| 500000.0| 4166.666666666667|\n",
      "|       3| 450000.0|            4500.0|\n",
      "+--------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result = spark.sql(\"\"\"\n",
    "SELECT bedrooms,\n",
    "       AVG(price) AS avg_price,\n",
    "       AVG(price_per_sqft) AS avg_price_per_sqft\n",
    "FROM houses\n",
    "WHERE bedrooms >= 3\n",
    "GROUP BY bedrooms\n",
    "ORDER BY avg_price DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc54909",
   "metadata": {},
   "source": [
    "## 7. Joins: Combining Multiple Tables\n",
    "\n",
    "Let's imagine we have a second table that contains information about neighborhoods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe59d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- neighborhood_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+---------------+---------+-----------+\n",
      "|neighborhood_id|     name|       city|\n",
      "+---------------+---------+-----------+\n",
      "|             10|  Central| Metropolis|\n",
      "|             11|Northside| Metropolis|\n",
      "|             12| Lakeside|Springfield|\n",
      "+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a small neighborhoods DataFrame\n",
    "neighborhood_data = [\n",
    "    Row(neighborhood_id=10, name=\"Central\", city=\"Metropolis\"),\n",
    "    Row(neighborhood_id=11, name=\"Northside\", city=\"Metropolis\"),\n",
    "    Row(neighborhood_id=12, name=\"Lakeside\", city=\"Springfield\"),\n",
    "]\\\n",
    "\n",
    "\n",
    "neigh_df = spark.createDataFrame(neighborhood_data)\n",
    "\n",
    "neigh_df.printSchema()\n",
    "neigh_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ab65a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "|neighborhood_id|house_id| price|sqft|bedrooms|year_built|price_per_bedroom|    price_per_sqft|     name|       city|\n",
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "|             10|       1|300000|  80|       2|      1990|           100000|            3750.0|  Central| Metropolis|\n",
      "|             10|       2|450000| 100|       3|      2005|           200000|            4500.0|  Central| Metropolis|\n",
      "|             11|       3|500000| 120|       4|      2010|           190000| 4166.666666666667|Northside| Metropolis|\n",
      "|             11|       4|200000|  60|       2|      1980|           180000|3333.3333333333335|Northside| Metropolis|\n",
      "|             12|       5|800000| 150|       5|      2018|           300000| 5333.333333333333| Lakeside|Springfield|\n",
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join houses with neighborhoods on neighborhood_id\n",
    "joined_df = houses_df.join(neigh_df, on=\"neighborhood_id\", how=\"left\")\n",
    "\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45379960",
   "metadata": {},
   "source": [
    "### Exercise: Neighborhood statistics\n",
    "\n",
    "Using `joined_df`:\n",
    "\n",
    "1. Compute the **average house price** per neighborhood `name`.\n",
    "2. Compute the average `price_per_sqft` per **city**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf95bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "|neighborhood_id|house_id| price|sqft|bedrooms|year_built|price_per_bedroom|    price_per_sqft|     name|       city|\n",
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "|             10|       1|300000|  80|       2|      1990|           100000|            3750.0|  Central| Metropolis|\n",
      "|             10|       2|450000| 100|       3|      2005|           200000|            4500.0|  Central| Metropolis|\n",
      "|             11|       3|500000| 120|       4|      2010|           190000| 4166.666666666667|Northside| Metropolis|\n",
      "|             11|       4|200000|  60|       2|      1980|           180000|3333.3333333333335|Northside| Metropolis|\n",
      "|             12|       5|800000| 150|       5|      2018|           300000| 5333.333333333333| Lakeside|Springfield|\n",
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = houses_df.join(neigh_df, on=\"neighborhood_id\", how=\"left\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed8948",
   "metadata": {},
   "source": [
    "## 8. A Mini ETL Pipeline in Spark\n",
    "\n",
    "We now put the pieces together into a simple **ETL (Extract–Transform–Load)** function:\n",
    "\n",
    "- **Extract:** Read data (in this tutorial we reuse the DataFrames created earlier).\n",
    "- **Transform:** Clean data, add derived columns, join tables.\n",
    "- **Load:** Write the result to disk (e.g., as Parquet).\n",
    "\n",
    "**Note:** For simplicity, this tutorial uses the in-memory sample data (`houses_df` and `neigh_df`) we created earlier. In a real-world scenario, you would read data from files using `spark.read.csv()` or `spark.read.parquet()`, transform it, and write the results back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a96d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "|neighborhood_id|house_id| price|sqft|bedrooms|year_built|price_per_bedroom|    price_per_sqft|     name|       city|\n",
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "|             10|       1|300000|  80|       2|      1990|         150000.0|            3750.0|  Central| Metropolis|\n",
      "|             10|       2|450000| 100|       3|      2005|         150000.0|            4500.0|  Central| Metropolis|\n",
      "|             11|       3|500000| 120|       4|      2010|         125000.0| 4166.666666666667|Northside| Metropolis|\n",
      "|             11|       4|200000|  60|       2|      1980|         100000.0|3333.3333333333335|Northside| Metropolis|\n",
      "|             12|       5|800000| 150|       5|      2018|         160000.0| 5333.333333333333| Lakeside|Springfield|\n",
      "+---------------+--------+------+----+--------+----------+-----------------+------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def transform_housing_data(spark):\n",
    "    \"\"\"Example ETL function for housing data.\n",
    "\n",
    "    In a real project, you would:\n",
    "    - Read houses from CSV or a database.\n",
    "    - Read neighborhoods from another source.\n",
    "    - Clean, transform, and join them.\n",
    "    - Write out the final DataFrame to Parquet for downstream BI tools.\n",
    "    \"\"\"\n",
    "\n",
    "    # Here we reuse houses_df and neigh_df from the notebook scope.\n",
    "    # In a standalone script, you would read them inside this function.\n",
    "    global houses_df, neigh_df\n",
    "\n",
    "    df = houses_df.dropna(subset=[\"price\", \"sqft\"])  # simple cleaning\n",
    "\n",
    "    df = df.withColumn(\"price_per_sqft\", col(\"price\") / col(\"sqft\"))\n",
    "    df = df.withColumn(\"price_per_bedroom\", col(\"price\") / col(\"bedrooms\"))\n",
    "\n",
    "    result = df.join(neigh_df, on=\"neighborhood_id\", how=\"left\")\n",
    "\n",
    "    # In a real setting, uncomment to write to disk:\n",
    "    # result.write.mode(\"overwrite\").parquet(\"output/cleaned_housing\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "etl_df = transform_housing_data(spark)\n",
    "etl_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a5bc6",
   "metadata": {},
   "source": [
    "### Exercise: Extend the ETL\n",
    "\n",
    "1. Add a new column `is_new` that is `True` if `year_built >= 2010`, else `False`.\n",
    "2. Aggregate `etl_df` to compute the average price for `is_new = True` vs `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0644e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|house_id|year_built|is_new|\n",
      "+--------+----------+------+\n",
      "|       1|      1990| false|\n",
      "|       2|      2005| false|\n",
      "|       3|      2010| false|\n",
      "|       4|      1980| false|\n",
      "|       5|      2018|  true|\n",
      "+--------+----------+------+\n",
      "\n",
      "+------+---------+\n",
      "|is_new|avg_price|\n",
      "+------+---------+\n",
      "|  true| 800000.0|\n",
      "| false| 362500.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add correct column\n",
    "extended_df = joined_df.withColumn(\n",
    "    \"is_new\",\n",
    "    F.col(\"year_built\") > 2010\n",
    ")\n",
    "\n",
    "# Preview\n",
    "extended_df.select(\"house_id\", \"year_built\", \"is_new\").show(10)\n",
    "\n",
    "# Average price for new vs old\n",
    "avg_price_df = (\n",
    "    extended_df.groupBy(\"is_new\")\n",
    "               .agg(F.avg(\"price\").alias(\"avg_price\"))\n",
    "               .orderBy(\"is_new\", ascending=False)\n",
    ")\n",
    "\n",
    "avg_price_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d01fe",
   "metadata": {},
   "source": [
    "## 9. Wrap-Up\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "- Started a **SparkSession** in local mode.\n",
    "- Created Spark **DataFrames** and inspected their schema and contents.\n",
    "- Performed **basic transformations** and **aggregations** using the DataFrame API.\n",
    "- Used **Spark SQL** queries on DataFrames.\n",
    "- Performed **joins** to combine multiple tables.\n",
    "- Built a small **ETL function** that transforms and combines data.\n",
    "\n",
    "### Where to go from here\n",
    "\n",
    "- Try reading real CSV or Parquet files with `spark.read.csv(...)` or `spark.read.parquet(...)`.\n",
    "- Experiment with **larger datasets** to see the benefit of Spark over Pandas.\n",
    "- Look into **Spark MLlib** for scalable machine learning.\n",
    "\n",
    "When you close your notebook, you can stop Spark with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84e18d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BONUS: Reading and Writing Files\n",
    "\n",
    "This section demonstrates how to read from CSV and write to Parquet files. **These cells are optional and can be deleted if you don't need file I/O examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30875984",
   "metadata": {},
   "source": [
    "### Writing to Parquet\n",
    "\n",
    "Parquet is a columnar storage format that's efficient for big data. Here's how to save your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Writing to Parquet format\n",
    "# etl_df.write.mode(\"overwrite\").parquet(\"../data/tmp/cleaned_housing_parquet\")\n",
    "\n",
    "# To read it back:\n",
    "# df_from_parquet = spark.read.parquet(\"../data/tmp/cleaned_housing_parquet\")\n",
    "# df_from_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf470d",
   "metadata": {},
   "source": [
    "### Reading from CSV\n",
    "\n",
    "Here's how you would read data from a CSV file in a real project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00107dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading a CSV file\n",
    "# df_from_csv = spark.read.csv(\n",
    "#     \"../data/your_file.csv\",\n",
    "#     header=True,        # First row contains column names\n",
    "#     inferSchema=True    # Automatically detect column types\n",
    "# )\n",
    "# df_from_csv.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
